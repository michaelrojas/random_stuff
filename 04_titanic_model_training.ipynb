{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition: Titanic\n",
    "Predicting whether an idividual survived or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',100)\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Import RandomForestClassifier and GradientBoostingClassifer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# Function for splitting training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Function for creating model pipelines\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# For standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Helper for cross-validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Classification metrics (added later)\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to follow: \n",
    "1. [Split dataset into train and test](#data_split)\n",
    "2. [Build the model pipelines](#pipeline)\n",
    "3. [Declare hyperparameters to tune for our task](#hyperparameters)\n",
    "4. [Fit and tune models using cross-validation](#tune)\n",
    "5. [Evaluate model](#evaluate)\n",
    "6. [Area under ROC curve](#auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('titanic_base_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 26)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>isFemale</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>alone</th>\n",
       "      <th>age_missing</th>\n",
       "      <th>group_size</th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "      <th>who_child</th>\n",
       "      <th>who_man</th>\n",
       "      <th>who_woman</th>\n",
       "      <th>deck_A</th>\n",
       "      <th>deck_B</th>\n",
       "      <th>deck_C</th>\n",
       "      <th>deck_D</th>\n",
       "      <th>deck_E</th>\n",
       "      <th>deck_F</th>\n",
       "      <th>deck_G</th>\n",
       "      <th>deck_Missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.45</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.75</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass  isFemale   age  sibsp  parch   fare  class  adult_male  \\\n",
       "888         0       3         1   0.0      1      2  23.45      3           0   \n",
       "889         1       1         0  26.0      0      0  30.00      1           1   \n",
       "890         0       3         0  32.0      0      0   7.75      3           1   \n",
       "\n",
       "     alone  age_missing  group_size  embarked_C  embarked_Q  embarked_S  \\\n",
       "888      0            1           4           0           0           1   \n",
       "889      1            0           1           1           0           0   \n",
       "890      1            0           1           0           1           0   \n",
       "\n",
       "     who_child  who_man  who_woman  deck_A  deck_B  deck_C  deck_D  deck_E  \\\n",
       "888          0        0          1       0       0       0       0       0   \n",
       "889          0        1          0       0       0       1       0       0   \n",
       "890          0        1          0       0       0       0       0       0   \n",
       "\n",
       "     deck_F  deck_G  deck_Missing  \n",
       "888       0       0             1  \n",
       "889       0       0             0  \n",
       "890       0       0             1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id='data_split'> </span>\n",
    "# Split Data into Test/Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891,)\n",
      "(891, 25)\n"
     ]
    }
   ],
   "source": [
    "y = df.survived\n",
    "\n",
    "X = df.drop('survived', axis=1)\n",
    "\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712 179 712 179\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42,\n",
    "                                                   stratify=df.survived)\n",
    "\n",
    "print(len(X_train),len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id='pipeline'></span>\n",
    "# Build Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models tested: \n",
    "* Logistic Regression w/ L1 Penalty\n",
    "* Logistic Regression w/ L2 Penalty\n",
    "* Random Forest\n",
    "* Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'l1':make_pipeline(StandardScaler(), \n",
    "                       LogisticRegression(penalty='l1', random_state=42,n_jobs=-1)),\n",
    "    'l2':make_pipeline(StandardScaler(), \n",
    "                       LogisticRegression(penalty='l2', random_state=42,n_jobs=-1)),\n",
    "    'rf':make_pipeline(StandardScaler(),\n",
    "                       RandomForestClassifier(random_state=42,n_jobs=-1)), \n",
    "    'gb':make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=42))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id='hyperparameters'></span>\n",
    "# Declare Hyperparameters to Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logisticregression': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l1', random_state=42, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " 'logisticregression__C': 1.0,\n",
       " 'logisticregression__class_weight': None,\n",
       " 'logisticregression__dual': False,\n",
       " 'logisticregression__fit_intercept': True,\n",
       " 'logisticregression__intercept_scaling': 1,\n",
       " 'logisticregression__max_iter': 100,\n",
       " 'logisticregression__multi_class': 'ovr',\n",
       " 'logisticregression__n_jobs': 1,\n",
       " 'logisticregression__penalty': 'l1',\n",
       " 'logisticregression__random_state': 42,\n",
       " 'logisticregression__solver': 'liblinear',\n",
       " 'logisticregression__tol': 0.0001,\n",
       " 'logisticregression__verbose': 0,\n",
       " 'logisticregression__warm_start': False,\n",
       " 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'standardscaler__copy': True,\n",
       " 'standardscaler__with_mean': True,\n",
       " 'standardscaler__with_std': True,\n",
       " 'steps': [('standardscaler',\n",
       "   StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('logisticregression',\n",
       "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "             penalty='l1', random_state=42, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False))]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipelines['l1'].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1_hyperparameters = {'logisticregression__C': np.linspace(1e-3, 1e3, 10)}\n",
    "l2_hyperparameters = {'logisticregression__C': np.linspace(1e-3, 1e3, 10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestclassifier': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
       "             verbose=0, warm_start=False),\n",
       " 'randomforestclassifier__bootstrap': True,\n",
       " 'randomforestclassifier__class_weight': None,\n",
       " 'randomforestclassifier__criterion': 'gini',\n",
       " 'randomforestclassifier__max_depth': None,\n",
       " 'randomforestclassifier__max_features': 'auto',\n",
       " 'randomforestclassifier__max_leaf_nodes': None,\n",
       " 'randomforestclassifier__min_impurity_split': 1e-07,\n",
       " 'randomforestclassifier__min_samples_leaf': 1,\n",
       " 'randomforestclassifier__min_samples_split': 2,\n",
       " 'randomforestclassifier__min_weight_fraction_leaf': 0.0,\n",
       " 'randomforestclassifier__n_estimators': 10,\n",
       " 'randomforestclassifier__n_jobs': -1,\n",
       " 'randomforestclassifier__oob_score': False,\n",
       " 'randomforestclassifier__random_state': 42,\n",
       " 'randomforestclassifier__verbose': 0,\n",
       " 'randomforestclassifier__warm_start': False,\n",
       " 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'standardscaler__copy': True,\n",
       " 'standardscaler__with_mean': True,\n",
       " 'standardscaler__with_std': True,\n",
       " 'steps': [('standardscaler',\n",
       "   StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('randomforestclassifier',\n",
       "   RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "               min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "               n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
       "               verbose=0, warm_start=False))]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipelines['rf'].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_hyperparameters = {'randomforestclassifier__n_estimators': [50, 100, 200],\n",
    "                     'randomforestclassifier__max_features': ['auto', 'sqrt', .33],\n",
    "                     'randomforestclassifier__max_depth': [3, 5, 8]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gradientboostingclassifier': GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "               learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "               min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "               n_estimators=100, presort='auto', random_state=42,\n",
       "               subsample=1.0, verbose=0, warm_start=False),\n",
       " 'gradientboostingclassifier__criterion': 'friedman_mse',\n",
       " 'gradientboostingclassifier__init': None,\n",
       " 'gradientboostingclassifier__learning_rate': 0.1,\n",
       " 'gradientboostingclassifier__loss': 'deviance',\n",
       " 'gradientboostingclassifier__max_depth': 3,\n",
       " 'gradientboostingclassifier__max_features': None,\n",
       " 'gradientboostingclassifier__max_leaf_nodes': None,\n",
       " 'gradientboostingclassifier__min_impurity_split': 1e-07,\n",
       " 'gradientboostingclassifier__min_samples_leaf': 1,\n",
       " 'gradientboostingclassifier__min_samples_split': 2,\n",
       " 'gradientboostingclassifier__min_weight_fraction_leaf': 0.0,\n",
       " 'gradientboostingclassifier__n_estimators': 100,\n",
       " 'gradientboostingclassifier__presort': 'auto',\n",
       " 'gradientboostingclassifier__random_state': 42,\n",
       " 'gradientboostingclassifier__subsample': 1.0,\n",
       " 'gradientboostingclassifier__verbose': 0,\n",
       " 'gradientboostingclassifier__warm_start': False,\n",
       " 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'standardscaler__copy': True,\n",
       " 'standardscaler__with_mean': True,\n",
       " 'standardscaler__with_std': True,\n",
       " 'steps': [('standardscaler',\n",
       "   StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('gradientboostingclassifier',\n",
       "   GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                 learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                 max_features=None, max_leaf_nodes=None,\n",
       "                 min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                 min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                 n_estimators=100, presort='auto', random_state=42,\n",
       "                 subsample=1.0, verbose=0, warm_start=False))]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipelines['gb'].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb_hyperparameters = {'gradientboostingclassifier__n_estimators': [50, 100, 200],\n",
    "                     'gradientboostingclassifier__learning_rate': [.05, .1, .2],\n",
    "                     'gradientboostingclassifier__max_depth': [3,5,8],\n",
    "                     'gradientboostingclassifier__max_features': ['auto','sqrt',.33]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'l1': l1_hyperparameters,\n",
    "    'l2': l2_hyperparameters,\n",
    "    'rf': rf_hyperparameters, \n",
    "    'gb': gb_hyperparameters\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id='tune'></span>\n",
    "# Fit and Tune Models Using Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 has been fitted.\n",
      "l2 has been fitted.\n",
      "rf has been fitted.\n",
      "gb has been fitted.\n",
      "CPU times: user 3.82 s, sys: 350 ms, total: 4.17 s\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "fitted_models = {}\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    #print('{}:: {}\\n\\n'.format(name, pipeline))\n",
    "    \n",
    "    # create cross validation object\n",
    "    model = GridSearchCV(pipeline, hyperparameters[name], cv=10, n_jobs=-1)\n",
    "    \n",
    "    # fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # store the model in the dictionary\n",
    "    fitted_models[name] = model\n",
    "    \n",
    "    print('{} has been fitted.'.format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id='evaluate'></span>\n",
    "# Evaluate the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 0.817415730337\n",
      "l2 0.817415730337\n",
      "rf 0.828651685393\n",
      "gb 0.824438202247\n"
     ]
    }
   ],
   "source": [
    "for k, v in fitted_models.items():\n",
    "    print(k, v.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('randomforestclassifier', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurit...imators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
       "            verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'randomforestclassifier__n_estimators': [50, 100, 200], 'randomforestclassifier__max_features': ['auto', 'sqrt', 0.33], 'randomforestclassifier__max_depth': [3, 5, 8]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_models['rf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id='auroc'></span>\n",
    "# Area Under the ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 has an auc score of 0.8600790513833992\n",
      "l2 has an auc score of 0.8598155467720685\n",
      "rf has an auc score of 0.8488801054018446\n",
      "gb has an auc score of 0.853227931488801\n"
     ]
    }
   ],
   "source": [
    "for name, model in fitted_models.items():\n",
    "    pred = model.predict_proba(X_test)\n",
    "    # get the 'survived' (1) class\n",
    "    pred = [p[1] for p in pred]\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred)\n",
    "    \n",
    "    print('{} has an auc score of {}'.format(name, auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression w/ L1 penalty gave us the best AUC score\n",
    "Save the L1 model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('titanic_final_model.pkl','wb') as f:\n",
    "    pickle.dump(fitted_models['l1'].best_estimator_,f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
